# {{ _('SkyPilot Task for Fine-Tuning with Unsloth') }}
# {{ _('Generated by Dumont Cloud Fine-Tuning Service') }}

name: {{ job_name }}

resources:
  cloud: vast
  accelerators: {{ gpu_type }}:{{ num_gpus }}
  disk_size: 200
  use_spot: true

{% if dataset_path.startswith('/') or dataset_path.startswith('~') %}
file_mounts:
  /dataset:
    source: {{ dataset_path }}
    mode: COPY
{% endif %}

setup: |
  set -e

  echo "{{ _('=== Setting up Unsloth Fine-Tuning Environment ===') }}"

  # {{ _('Install Unsloth and dependencies') }}
  pip install --quiet "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
  pip install --quiet --no-deps trl peft accelerate bitsandbytes
  pip install --quiet datasets transformers torch xformers

  # {{ _('Create output directory') }}
  mkdir -p {{ output_dir }}

  # {{ _('Pre-download model to cache') }}
  echo "{% trans %}Pre-downloading base model:{% endtrans %} {{ base_model }}"
  python -c "from unsloth import FastLanguageModel; FastLanguageModel.from_pretrained('{{ base_model }}', max_seq_length={{ max_seq_length }}, load_in_4bit=True)"

  echo "{{ _('=== Setup Complete ===') }}"

run: |
  set -e

  echo "{{ _('=== Starting Fine-Tuning with Unsloth ===') }}"
  echo "{% trans %}Base Model:{% endtrans %} {{ base_model }}"
  echo "{% trans %}LoRA Rank:{% endtrans %} {{ lora_rank }}"
  echo "{% trans %}Learning Rate:{% endtrans %} {{ learning_rate }}"
  echo "{% trans %}Epochs:{% endtrans %} {{ epochs }}"
  echo "{% trans %}Batch Size:{% endtrans %} {{ batch_size }}"
  echo "{% trans %}Max Sequence Length:{% endtrans %} {{ max_seq_length }}"

  python - << 'FINETUNE_SCRIPT'
import os
import json
import torch
from datetime import datetime
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

# {{ _('Configuration from template') }}
BASE_MODEL = "{{ base_model }}"
LORA_RANK = {{ lora_rank }}
LORA_ALPHA = {{ lora_alpha }}
LEARNING_RATE = {{ learning_rate }}
EPOCHS = {{ epochs }}
BATCH_SIZE = {{ batch_size }}
GRADIENT_ACCUMULATION = {{ gradient_accumulation_steps }}
MAX_SEQ_LENGTH = {{ max_seq_length }}
WARMUP_STEPS = {{ warmup_steps }}
WEIGHT_DECAY = {{ weight_decay }}
OUTPUT_DIR = "{{ output_dir }}"
DATASET_FORMAT = "{{ dataset_format }}"

print(f"\n{'='*60}")
print(f"{{ _('DUMONT CLOUD FINE-TUNING JOB') }}")
print(f"{% trans %}Started at:{% endtrans %} {datetime.now().isoformat()}")
print(f"{'='*60}\n")

# {{ _('Load model with Unsloth (4-bit quantization)') }}
print(f"{% trans %}Loading model:{% endtrans %} {BASE_MODEL}")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=BASE_MODEL,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,  # {{ _('Auto-detect (float16 or bfloat16)') }}
    load_in_4bit=True,
)
print("{{ _('Model loaded successfully!') }}")

# {{ _('Add LoRA adapters') }}
print(f"{% trans %}Adding LoRA adapters (rank={% endtrans %}{LORA_RANK}{% trans %}, alpha={% endtrans %}{LORA_ALPHA})")
model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_RANK,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=LORA_ALPHA,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)
print("{{ _('LoRA adapters added!') }}")

# {{ _('Load dataset') }}
{% if dataset_path.startswith('http') %}
# {{ _('Load from URL') }}
print(f"{% trans %}Loading dataset from URL:{% endtrans %} {{ dataset_path }}")
dataset = load_dataset("json", data_files="{{ dataset_path }}", split="train")
{% elif dataset_path.startswith('~') or dataset_path.startswith('/') %}
# {{ _('Load from local file') }}
print("{% trans %}Loading dataset from local file:{% endtrans %} /dataset")
dataset_files = []
for f in os.listdir("/dataset"):
    if f.endswith(('.json', '.jsonl')):
        dataset_files.append(f"/dataset/{f}")

if not dataset_files:
    raise ValueError("{{ _('No JSON/JSONL files found in dataset directory') }}")

print(f"{% trans %}Found dataset files:{% endtrans %} {dataset_files}")
dataset = load_dataset("json", data_files=dataset_files, split="train")
{% else %}
# {{ _('Try to load from HuggingFace') }}
print(f"{% trans %}Loading dataset from HuggingFace:{% endtrans %} {{ dataset_path }}")
dataset = load_dataset("{{ dataset_path }}", split="train")
{% endif %}

print(f"{% trans %}Dataset loaded:{% endtrans %} {len(dataset)} {% trans %}examples{% endtrans %}")

# {{ _('Format dataset based on format type') }}
if DATASET_FORMAT == "alpaca":
    def format_prompts(examples):
        """{{ _('Format Alpaca-style dataset') }}"""
        texts = []
        for i in range(len(examples.get("instruction", []))):
            instruction = examples["instruction"][i]
            input_text = examples.get("input", [""] * len(examples["instruction"]))[i]
            output = examples["output"][i]

            if input_text:
                text = f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
{output}"""
            else:
                text = f"""### Instruction:
{instruction}

### Response:
{output}"""
            texts.append(text + tokenizer.eos_token)
        return {"text": texts}

    dataset = dataset.map(format_prompts, batched=True)

elif DATASET_FORMAT == "sharegpt":
    def format_sharegpt(examples):
        """{{ _('Format ShareGPT-style dataset') }}"""
        texts = []
        for convs in examples.get("conversations", []):
            text = ""
            for msg in convs:
                role = msg.get("from", msg.get("role", ""))
                content = msg.get("value", msg.get("content", ""))
                if role in ["human", "user"]:
                    text += f"### Human:\n{content}\n\n"
                elif role in ["gpt", "assistant"]:
                    text += f"### Assistant:\n{content}\n\n"
            texts.append(text.strip() + tokenizer.eos_token)
        return {"text": texts}

    dataset = dataset.map(format_sharegpt, batched=True)

else:
    # {{ _('Assume raw text format') }}
    if "text" not in dataset.column_names:
        raise ValueError("{{ _('Dataset must have text column for raw format') }}")

print(f"{% trans %}Dataset formatted with{% endtrans %} {len(dataset)} {% trans %}examples{% endtrans %}")

# {{ _('Training arguments') }}
training_args = TrainingArguments(
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    warmup_steps=WARMUP_STEPS,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    logging_steps=1,
    optim="adamw_8bit",
    weight_decay=WEIGHT_DECAY,
    lr_scheduler_type="linear",
    seed=3407,
    output_dir=OUTPUT_DIR,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none",
    dataloader_num_workers=2,
)

# {{ _('Create trainer') }}
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    dataset_num_proc=2,
    packing=False,
    args=training_args,
)

# {{ _('Train!') }}
print("\n" + "="*60)
print("{{ _('STARTING TRAINING') }}")
print("="*60 + "\n")

trainer_stats = trainer.train()

print("\n" + "="*60)
print("{{ _('TRAINING COMPLETE!') }}")
print("="*60 + "\n")

# {{ _('Save LoRA adapters') }}
lora_output = os.path.join(OUTPUT_DIR, "lora_model")
print(f"{% trans %}Saving LoRA adapters to:{% endtrans %} {lora_output}")
model.save_pretrained(lora_output)
tokenizer.save_pretrained(lora_output)

# {{ _('Save training stats') }}
stats_file = os.path.join(OUTPUT_DIR, "training_stats.json")
stats = {
    "train_loss": trainer_stats.training_loss,
    "train_runtime": trainer_stats.metrics.get("train_runtime", 0),
    "train_samples_per_second": trainer_stats.metrics.get("train_samples_per_second", 0),
    "train_steps_per_second": trainer_stats.metrics.get("train_steps_per_second", 0),
    "total_flos": trainer_stats.metrics.get("total_flos", 0),
    "completed_at": datetime.now().isoformat(),
    "config": {
        "base_model": BASE_MODEL,
        "lora_rank": LORA_RANK,
        "lora_alpha": LORA_ALPHA,
        "learning_rate": LEARNING_RATE,
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "max_seq_length": MAX_SEQ_LENGTH,
    }
}

with open(stats_file, "w") as f:
    json.dump(stats, f, indent=2)

print(f"\n{{ _('Training Statistics:') }}")
print(f"  - {% trans %}Final Loss:{% endtrans %} {trainer_stats.training_loss:.4f}")
print(f"  - {% trans %}Runtime:{% endtrans %} {trainer_stats.metrics.get('train_runtime', 0):.2f}s")
print(f"  - {% trans %}Samples/sec:{% endtrans %} {trainer_stats.metrics.get('train_samples_per_second', 0):.2f}")

print(f"\n{'='*60}")
print(f"{{ _('FINE-TUNING COMPLETE!') }}")
print(f"{% trans %}Model saved to:{% endtrans %} {lora_output}")
print(f"{% trans %}Stats saved to:{% endtrans %} {stats_file}")
print(f"{'='*60}\n")

FINETUNE_SCRIPT

  echo "{{ _('=== Fine-Tuning Job Complete ===') }}"
  ls -la {{ output_dir }}/
