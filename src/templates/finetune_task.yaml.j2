# SkyPilot Task for Fine-Tuning with Unsloth
# Generated by Dumont Cloud Fine-Tuning Service

name: {{ job_name }}

resources:
  cloud: vast
  accelerators: {{ gpu_type }}:{{ num_gpus }}
  disk_size: 200
  use_spot: true

{% if dataset_path.startswith('/') or dataset_path.startswith('~') %}
file_mounts:
  /dataset:
    source: {{ dataset_path }}
    mode: COPY
{% endif %}

setup: |
  set -e

  echo "=== Setting up Unsloth Fine-Tuning Environment ==="

  # Install Unsloth and dependencies
  pip install --quiet "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
  pip install --quiet --no-deps trl peft accelerate bitsandbytes
  pip install --quiet datasets transformers torch xformers

  # Create output directory
  mkdir -p {{ output_dir }}

  # Pre-download model to cache
  echo "Pre-downloading base model: {{ base_model }}"
  python -c "from unsloth import FastLanguageModel; FastLanguageModel.from_pretrained('{{ base_model }}', max_seq_length={{ max_seq_length }}, load_in_4bit=True)"

  echo "=== Setup Complete ==="

run: |
  set -e

  echo "=== Starting Fine-Tuning with Unsloth ==="
  echo "Base Model: {{ base_model }}"
  echo "LoRA Rank: {{ lora_rank }}"
  echo "Learning Rate: {{ learning_rate }}"
  echo "Epochs: {{ epochs }}"
  echo "Batch Size: {{ batch_size }}"
  echo "Max Sequence Length: {{ max_seq_length }}"

  python - << 'FINETUNE_SCRIPT'
import os
import json
import torch
from datetime import datetime
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset

# Configuration from template
BASE_MODEL = "{{ base_model }}"
LORA_RANK = {{ lora_rank }}
LORA_ALPHA = {{ lora_alpha }}
LEARNING_RATE = {{ learning_rate }}
EPOCHS = {{ epochs }}
BATCH_SIZE = {{ batch_size }}
GRADIENT_ACCUMULATION = {{ gradient_accumulation_steps }}
MAX_SEQ_LENGTH = {{ max_seq_length }}
WARMUP_STEPS = {{ warmup_steps }}
WEIGHT_DECAY = {{ weight_decay }}
OUTPUT_DIR = "{{ output_dir }}"
DATASET_FORMAT = "{{ dataset_format }}"

print(f"\n{'='*60}")
print(f"DUMONT CLOUD FINE-TUNING JOB")
print(f"Started at: {datetime.now().isoformat()}")
print(f"{'='*60}\n")

# Load model with Unsloth (4-bit quantization)
print(f"Loading model: {BASE_MODEL}")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=BASE_MODEL,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,  # Auto-detect (float16 or bfloat16)
    load_in_4bit=True,
)
print("Model loaded successfully!")

# Add LoRA adapters
print(f"Adding LoRA adapters (rank={LORA_RANK}, alpha={LORA_ALPHA})")
model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_RANK,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=LORA_ALPHA,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
)
print("LoRA adapters added!")

# Load dataset
{% if dataset_path.startswith('http') %}
# Load from URL
print(f"Loading dataset from URL: {{ dataset_path }}")
dataset = load_dataset("json", data_files="{{ dataset_path }}", split="train")
{% elif dataset_path.startswith('~') or dataset_path.startswith('/') %}
# Load from local file
print("Loading dataset from local file: /dataset")
dataset_files = []
for f in os.listdir("/dataset"):
    if f.endswith(('.json', '.jsonl')):
        dataset_files.append(f"/dataset/{f}")

if not dataset_files:
    raise ValueError("No JSON/JSONL files found in dataset directory")

print(f"Found dataset files: {dataset_files}")
dataset = load_dataset("json", data_files=dataset_files, split="train")
{% else %}
# Try to load from HuggingFace
print(f"Loading dataset from HuggingFace: {{ dataset_path }}")
dataset = load_dataset("{{ dataset_path }}", split="train")
{% endif %}

print(f"Dataset loaded: {len(dataset)} examples")

# Format dataset based on format type
if DATASET_FORMAT == "alpaca":
    def format_prompts(examples):
        """Format Alpaca-style dataset"""
        texts = []
        for i in range(len(examples.get("instruction", []))):
            instruction = examples["instruction"][i]
            input_text = examples.get("input", [""] * len(examples["instruction"]))[i]
            output = examples["output"][i]

            if input_text:
                text = f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
{output}"""
            else:
                text = f"""### Instruction:
{instruction}

### Response:
{output}"""
            texts.append(text + tokenizer.eos_token)
        return {"text": texts}

    dataset = dataset.map(format_prompts, batched=True)

elif DATASET_FORMAT == "sharegpt":
    def format_sharegpt(examples):
        """Format ShareGPT-style dataset"""
        texts = []
        for convs in examples.get("conversations", []):
            text = ""
            for msg in convs:
                role = msg.get("from", msg.get("role", ""))
                content = msg.get("value", msg.get("content", ""))
                if role in ["human", "user"]:
                    text += f"### Human:\n{content}\n\n"
                elif role in ["gpt", "assistant"]:
                    text += f"### Assistant:\n{content}\n\n"
            texts.append(text.strip() + tokenizer.eos_token)
        return {"text": texts}

    dataset = dataset.map(format_sharegpt, batched=True)

else:
    # Assume raw text format
    if "text" not in dataset.column_names:
        raise ValueError("Dataset must have 'text' column for raw format")

print(f"Dataset formatted with {len(dataset)} examples")

# Training arguments
training_args = TrainingArguments(
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    warmup_steps=WARMUP_STEPS,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    logging_steps=1,
    optim="adamw_8bit",
    weight_decay=WEIGHT_DECAY,
    lr_scheduler_type="linear",
    seed=3407,
    output_dir=OUTPUT_DIR,
    save_strategy="epoch",
    save_total_limit=2,
    report_to="none",
    dataloader_num_workers=2,
)

# Create trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    dataset_num_proc=2,
    packing=False,
    args=training_args,
)

# Train!
print("\n" + "="*60)
print("STARTING TRAINING")
print("="*60 + "\n")

trainer_stats = trainer.train()

print("\n" + "="*60)
print("TRAINING COMPLETE!")
print("="*60 + "\n")

# Save LoRA adapters
lora_output = os.path.join(OUTPUT_DIR, "lora_model")
print(f"Saving LoRA adapters to: {lora_output}")
model.save_pretrained(lora_output)
tokenizer.save_pretrained(lora_output)

# Save training stats
stats_file = os.path.join(OUTPUT_DIR, "training_stats.json")
stats = {
    "train_loss": trainer_stats.training_loss,
    "train_runtime": trainer_stats.metrics.get("train_runtime", 0),
    "train_samples_per_second": trainer_stats.metrics.get("train_samples_per_second", 0),
    "train_steps_per_second": trainer_stats.metrics.get("train_steps_per_second", 0),
    "total_flos": trainer_stats.metrics.get("total_flos", 0),
    "completed_at": datetime.now().isoformat(),
    "config": {
        "base_model": BASE_MODEL,
        "lora_rank": LORA_RANK,
        "lora_alpha": LORA_ALPHA,
        "learning_rate": LEARNING_RATE,
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "max_seq_length": MAX_SEQ_LENGTH,
    }
}

with open(stats_file, "w") as f:
    json.dump(stats, f, indent=2)

print(f"\nTraining Statistics:")
print(f"  - Final Loss: {trainer_stats.training_loss:.4f}")
print(f"  - Runtime: {trainer_stats.metrics.get('train_runtime', 0):.2f}s")
print(f"  - Samples/sec: {trainer_stats.metrics.get('train_samples_per_second', 0):.2f}")

print(f"\n{'='*60}")
print(f"FINE-TUNING COMPLETE!")
print(f"Model saved to: {lora_output}")
print(f"Stats saved to: {stats_file}")
print(f"{'='*60}\n")

FINETUNE_SCRIPT

  echo "=== Fine-Tuning Job Complete ==="
  ls -la {{ output_dir }}/
