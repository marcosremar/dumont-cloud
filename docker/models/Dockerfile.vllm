# vLLM Server for Large Language Models
# Image: dumontcloud/vllm:0.6.2-cuda12.1
#
# Build:
#   docker build -f Dockerfile.vllm -t dumontcloud/vllm:0.6.2-cuda12.1 .
#
# Run:
#   docker run --gpus all -e MODEL_ID="meta-llama/Llama-3.2-1B-Instruct" -p 8000:8000 dumontcloud/vllm:0.6.2-cuda12.1

FROM vllm/vllm-openai:v0.6.2

WORKDIR /app

# vLLM image already has everything, just set defaults
ENV PORT=8000
ENV MODEL_ID="meta-llama/Llama-3.2-1B-Instruct"
ENV MAX_MODEL_LEN=4096
ENV GPU_MEMORY_UTILIZATION=0.9

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

EXPOSE 8000

# vLLM entrypoint with OpenAI-compatible API
CMD python -m vllm.entrypoints.openai.api_server \
    --model $MODEL_ID \
    --port $PORT \
    --host 0.0.0.0 \
    --max-model-len $MAX_MODEL_LEN \
    --gpu-memory-utilization $GPU_MEMORY_UTILIZATION \
    --trust-remote-code
