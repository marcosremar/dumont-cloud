{
  "integrations_researched": [
    {
      "name": "JupyterLab",
      "type": "application",
      "purpose": "General ML development environment with notebook interface",
      "verified_package": {
        "name": "jupyterlab",
        "install_command": "pip install jupyterlab",
        "docker_images": [
          "jupyter/scipy-notebook",
          "jupyter/tensorflow-notebook",
          "jupyter/pytorch-notebook"
        ],
        "recommended_image": "jupyter/pytorch-notebook:latest",
        "verified": false
      },
      "gpu_requirements": {
        "cuda_required": false,
        "cuda_optional": true,
        "minimum_vram": "4GB (for ML workflows)",
        "notes": "Base Jupyter doesn't need GPU, but ML libraries (PyTorch, TensorFlow) do"
      },
      "api_patterns": {
        "launch_command": "jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root",
        "key_configs": [
          "NotebookApp.token",
          "NotebookApp.password",
          "NotebookApp.allow_origin"
        ],
        "verified_against": "Industry standard knowledge (not verified live)"
      },
      "configuration": {
        "env_vars": [
          "JUPYTER_ENABLE_LAB=yes",
          "GRANT_SUDO=yes (optional)"
        ],
        "ports": [8888],
        "volumes": ["/home/jovyan/work"]
      },
      "infrastructure": {
        "requires_docker": true,
        "base_image": "jupyter/base-notebook",
        "gpu_image": "tensorflow/tensorflow:latest-gpu-jupyter or pytorch/pytorch:latest",
        "typical_size": "2-5GB"
      },
      "gotchas": [
        "Must set --allow-root when running as root in containers",
        "Default token authentication needs to be configured or disabled",
        "Jupyter official images run as non-root user 'jovyan' by default",
        "CUDA libraries must be installed separately or use GPU-enabled base images",
        "Port 8888 must be exposed for web access"
      ],
      "research_sources": [
        "https://jupyter-docker-stacks.readthedocs.io/ (not accessed - needs verification)",
        "https://github.com/jupyter/docker-stacks (not accessed - needs verification)"
      ],
      "boot_time_estimate": "30-60 seconds (without GPU libs), 60-120 seconds (with GPU libs)"
    },
    {
      "name": "Stable Diffusion WebUI (AUTOMATIC1111)",
      "type": "application",
      "purpose": "Web interface for AI image generation using Stable Diffusion models",
      "verified_package": {
        "name": "stable-diffusion-webui",
        "source": "https://github.com/AUTOMATIC1111/stable-diffusion-webui",
        "install_method": "git clone",
        "docker_images": [
          "ghcr.io/absolutelyludicrous/automatic1111-webui:latest"
        ],
        "verified": false
      },
      "gpu_requirements": {
        "cuda_required": true,
        "minimum_cuda_version": "11.8",
        "minimum_vram": "4GB (SD 1.5), 8GB+ (SDXL)",
        "recommended_gpu": "RTX 3060 12GB or better",
        "notes": "Will run on CPU but extremely slow (30+ seconds per image)"
      },
      "api_patterns": {
        "launch_command": "python launch.py --listen --port 7860",
        "key_flags": [
          "--listen (bind to 0.0.0.0)",
          "--port (default 7860)",
          "--xformers (memory optimization)",
          "--medvram or --lowvram (VRAM optimization)",
          "--api (enable REST API)"
        ],
        "verified_against": "Industry standard knowledge (not verified live)"
      },
      "configuration": {
        "env_vars": [
          "COMMANDLINE_ARGS (for launch flags)",
          "PYTORCH_CUDA_ALLOC_CONF (for memory management)"
        ],
        "ports": [7860],
        "volumes": [
          "/app/models (for Stable Diffusion checkpoints)",
          "/app/outputs (for generated images)"
        ],
        "typical_model_size": "2-7GB per checkpoint"
      },
      "infrastructure": {
        "requires_docker": false,
        "docker_recommended": true,
        "base_image": "pytorch/pytorch:2.0.1-cuda11.8-cudnn8-runtime",
        "typical_image_size": "15-20GB (with models)"
      },
      "dependencies": {
        "system": [
          "python 3.10",
          "git",
          "wget"
        ],
        "python": [
          "torch>=2.0.0+cu118",
          "torchvision",
          "xformers (optional, for memory optimization)"
        ]
      },
      "gotchas": [
        "First launch downloads ~4GB of model files",
        "Requires --listen flag to be accessible outside localhost",
        "xformers must match PyTorch CUDA version exactly",
        "Models must be placed in specific directories",
        "CUDA toolkit must be installed in container",
        "OutOfMemory errors common - requires --medvram or --lowvram flags",
        "Some extensions may break between updates"
      ],
      "research_sources": [
        "https://github.com/AUTOMATIC1111/stable-diffusion-webui (not accessed - needs verification)"
      ],
      "boot_time_estimate": "90-120 seconds (models pre-downloaded), 5+ minutes (first boot)"
    },
    {
      "name": "ComfyUI",
      "type": "application",
      "purpose": "Node-based workflow interface for Stable Diffusion and AI image generation",
      "verified_package": {
        "name": "ComfyUI",
        "source": "https://github.com/comfyanonymous/ComfyUI",
        "install_method": "git clone",
        "docker_images": [
          "yanwk/comfyui-boot:latest"
        ],
        "verified": false
      },
      "gpu_requirements": {
        "cuda_required": true,
        "minimum_cuda_version": "11.8",
        "minimum_vram": "4GB minimum, 8GB+ recommended",
        "recommended_gpu": "RTX 3060 12GB or better",
        "notes": "More memory-efficient than A1111 due to model loading strategy"
      },
      "api_patterns": {
        "launch_command": "python main.py --listen 0.0.0.0 --port 8188",
        "key_flags": [
          "--listen (bind address)",
          "--port (default 8188)",
          "--lowvram",
          "--normalvram",
          "--highvram"
        ],
        "api_endpoint": "http://localhost:8188/api",
        "verified_against": "Industry standard knowledge (not verified live)"
      },
      "configuration": {
        "env_vars": [],
        "ports": [8188],
        "volumes": [
          "/app/models (shared with SD models)",
          "/app/output (generated images)",
          "/app/input (input images)",
          "/app/custom_nodes (extensions)"
        ]
      },
      "infrastructure": {
        "requires_docker": false,
        "docker_recommended": true,
        "base_image": "pytorch/pytorch:2.0.1-cuda11.8-cudnn8-runtime",
        "typical_image_size": "10-15GB (with models)"
      },
      "dependencies": {
        "system": [
          "python 3.10+",
          "git"
        ],
        "python": [
          "torch>=2.0.0",
          "torchvision",
          "torchaudio"
        ]
      },
      "gotchas": [
        "Models directory structure must match expected paths",
        "Custom nodes are community-maintained and may break",
        "Workflow JSON files can become incompatible between versions",
        "Requires --listen 0.0.0.0 for external access",
        "More complex for beginners than A1111 WebUI",
        "Some workflows require specific custom nodes"
      ],
      "research_sources": [
        "https://github.com/comfyanonymous/ComfyUI (not accessed - needs verification)"
      ],
      "boot_time_estimate": "60-90 seconds"
    },
    {
      "name": "vLLM",
      "type": "library",
      "purpose": "High-performance LLM inference server with PagedAttention",
      "verified_package": {
        "name": "vllm",
        "install_command": "pip install vllm",
        "docker_images": [
          "vllm/vllm-openai:latest"
        ],
        "verified": false
      },
      "gpu_requirements": {
        "cuda_required": true,
        "minimum_cuda_version": "11.8",
        "minimum_vram": "16GB (7B models), 24GB+ (13B models), 40GB+ (70B models)",
        "recommended_gpu": "A100 40GB or H100 for production",
        "notes": "PagedAttention reduces memory usage vs standard inference"
      },
      "api_patterns": {
        "launch_command": "python -m vllm.entrypoints.openai.api_server --model <model_name> --host 0.0.0.0 --port 8000",
        "imports": [
          "from vllm import LLM, SamplingParams"
        ],
        "initialization": "llm = LLM(model='meta-llama/Llama-2-7b-hf')",
        "key_functions": [
          "llm.generate(prompts, sampling_params)",
          "AsyncLLMEngine for async serving"
        ],
        "api_compatibility": "OpenAI-compatible API endpoints",
        "verified_against": "Industry standard knowledge (not verified live)"
      },
      "configuration": {
        "env_vars": [
          "HUGGING_FACE_HUB_TOKEN (for gated models)",
          "VLLM_USE_MODELSCOPE (alternative model source)",
          "CUDA_VISIBLE_DEVICES (GPU selection)"
        ],
        "ports": [8000],
        "volumes": [
          "/root/.cache/huggingface (model cache)"
        ],
        "key_flags": [
          "--model (required)",
          "--tensor-parallel-size (multi-GPU)",
          "--max-num-batched-tokens",
          "--max-num-seqs",
          "--gpu-memory-utilization (default 0.9)"
        ]
      },
      "infrastructure": {
        "requires_docker": false,
        "docker_recommended": true,
        "base_image": "nvidia/cuda:12.1.0-devel-ubuntu22.04",
        "typical_image_size": "10-15GB (without models)"
      },
      "dependencies": {
        "system": [
          "python 3.8-3.11",
          "CUDA 11.8 or 12.1",
          "cuDNN"
        ],
        "python": [
          "torch>=2.1.0",
          "transformers",
          "ray (for distributed inference)",
          "fastapi",
          "uvicorn"
        ]
      },
      "gotchas": [
        "Model must fit in GPU VRAM (no CPU offloading)",
        "First run downloads model from HuggingFace (can be GBs)",
        "Requires specific PyTorch + CUDA version combinations",
        "Tensor parallelism requires identical GPUs",
        "Some model architectures not supported (check compatibility matrix)",
        "High memory usage during model loading",
        "Quantization (AWQ, GPTQ) reduces memory but needs compatible models"
      ],
      "research_sources": [
        "https://docs.vllm.ai/ (not accessed - needs verification)",
        "https://github.com/vllm-project/vllm (not accessed - needs verification)"
      ],
      "boot_time_estimate": "120-180 seconds (depends on model size and download)"
    },
    {
      "name": "CUDA",
      "type": "infrastructure",
      "purpose": "NVIDIA GPU compute platform required for all GPU workloads",
      "verified_package": {
        "name": "cuda-toolkit",
        "docker_images": [
          "nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04",
          "nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04"
        ],
        "verified": false
      },
      "versions": {
        "cuda_11.8": "Most compatible with PyTorch 2.0+",
        "cuda_12.1": "Newer, better performance but less compatibility",
        "recommended": "11.8 for maximum compatibility"
      },
      "configuration": {
        "env_vars": [
          "CUDA_VISIBLE_DEVICES",
          "NVIDIA_VISIBLE_DEVICES",
          "NVIDIA_DRIVER_CAPABILITIES=compute,utility"
        ],
        "docker_runtime": "nvidia (required for GPU access)"
      },
      "infrastructure": {
        "requires_docker": true,
        "docker_flag": "--gpus all or --gpus device=0",
        "nvidia_docker_required": true
      },
      "gotchas": [
        "CUDA version must match PyTorch compilation version",
        "Driver version on host must be >= CUDA version in container",
        "nvidia-docker2 or nvidia-container-toolkit must be installed on host",
        "CUDA toolkit inside container must match workload requirements",
        "cuDNN version compatibility is critical for deep learning",
        "Different tools may require different CUDA versions (version conflicts)"
      ],
      "compatibility_matrix": {
        "pytorch_2.0": "CUDA 11.7, 11.8",
        "pytorch_2.1": "CUDA 11.8, 12.1",
        "tensorflow_2.13": "CUDA 11.8",
        "tensorflow_2.14": "CUDA 11.8, 12.2"
      },
      "research_sources": [
        "https://docs.nvidia.com/cuda/ (not accessed - needs verification)",
        "https://developer.nvidia.com/cuda-toolkit (not accessed - needs verification)"
      ]
    },
    {
      "name": "Docker",
      "type": "infrastructure",
      "purpose": "Container platform for template packaging and deployment",
      "verified_package": {
        "name": "docker",
        "verified": false
      },
      "gpu_requirements": {
        "nvidia_docker_required": true,
        "packages": [
          "nvidia-docker2",
          "nvidia-container-toolkit"
        ]
      },
      "configuration": {
        "docker_compose_example": {
          "version": "3.8",
          "services": {
            "template": {
              "runtime": "nvidia",
              "environment": [
                "NVIDIA_VISIBLE_DEVICES=all"
              ]
            }
          }
        }
      },
      "gotchas": [
        "Default Docker runtime doesn't support GPUs",
        "Must install nvidia-docker2 or nvidia-container-toolkit",
        "Must use --gpus flag or runtime: nvidia in docker-compose",
        "Large image sizes (10-20GB) require storage planning",
        "Layer caching important for build performance"
      ],
      "research_sources": [
        "https://docs.docker.com/ (not accessed - needs verification)",
        "https://github.com/NVIDIA/nvidia-docker (not accessed - needs verification)"
      ]
    },
    {
      "name": "Vast.ai",
      "type": "service",
      "purpose": "GPU marketplace platform - target deployment infrastructure",
      "verified_package": {
        "name": "vast.ai",
        "api_available": "unknown",
        "verified": false
      },
      "integration_requirements": {
        "deployment_method": "unknown - needs exploration phase investigation",
        "template_format": "unknown - likely Docker images",
        "instance_provisioning_api": "unknown - needs verification",
        "gpu_metadata_access": "unknown - needed for recommendations"
      },
      "critical_unknowns": [
        "How does Vast.ai currently provision instances?",
        "Is there an API for programmatic deployment?",
        "What image formats are supported (Docker, bare metal)?",
        "How are existing templates/images managed?",
        "What metadata is available about GPUs for recommendations?",
        "What is the current baseline instance boot time?",
        "How is network/port configuration handled?",
        "Is there existing template infrastructure to extend?"
      ],
      "research_sources": [
        "https://vast.ai/docs/ (not accessed - needs verification)",
        "Internal codebase - needs exploration phase investigation"
      ]
    }
  ],
  "unverified_claims": [
    {
      "claim": "All package names and Docker images listed",
      "reason": "Web research tools not accessible - based on industry knowledge",
      "risk_level": "low",
      "mitigation": "Package names are well-known and standard. Verify during exploration phase."
    },
    {
      "claim": "API patterns and configuration options",
      "reason": "Could not access official documentation",
      "risk_level": "low",
      "mitigation": "Based on widely-used patterns. Verify specific flags/options during implementation."
    },
    {
      "claim": "Boot time estimates",
      "reason": "Estimates based on typical hardware, not tested",
      "risk_level": "medium",
      "mitigation": "Actual boot times depend on GPU, network speed, pre-downloaded models. Test in actual Vast.ai environment."
    },
    {
      "claim": "Vast.ai integration capabilities",
      "reason": "No access to Vast.ai documentation or existing codebase",
      "risk_level": "high",
      "mitigation": "CRITICAL: Exploration phase must investigate existing infrastructure and deployment mechanisms."
    }
  ],
  "recommendations": [
    {
      "category": "Image Strategy",
      "recommendation": "Pre-build Docker images with all dependencies",
      "rationale": "Downloading models and installing packages at boot time will exceed 2-minute target. Pre-built images with embedded models are essential."
    },
    {
      "category": "CUDA Compatibility",
      "recommendation": "Standardize on CUDA 11.8 for all templates",
      "rationale": "CUDA 11.8 offers best compatibility across PyTorch 2.0+, TensorFlow 2.13+, and most ML libraries. Reduces version conflict issues."
    },
    {
      "category": "Base Image Selection",
      "recommendation": "Use nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 as common base",
      "rationale": "Runtime image (vs devel) is smaller. Ubuntu 22.04 LTS provides stability. CUDA 11.8 + cuDNN 8 widely compatible."
    },
    {
      "category": "Model Pre-loading",
      "recommendation": "Embed default models in Docker images",
      "rationale": "For SD WebUI (4GB) and vLLM (2-20GB), downloading models at boot time will exceed time budget. Trade image size for boot speed."
    },
    {
      "category": "GPU Recommendations",
      "recommendation": "Embed minimum/recommended GPU specs in template metadata",
      "rationale": "Jupyter: 4GB+ VRAM, SD WebUI: 8GB+ (SDXL), ComfyUI: 8GB+, vLLM: 16GB+ (7B model). Users need clear guidance."
    },
    {
      "category": "Documentation",
      "recommendation": "Create per-template getting-started guides covering: first access, default credentials, common workflows, troubleshooting",
      "rationale": "Acceptance criteria requires documentation. Critical for 'one-click' UX promise."
    },
    {
      "category": "Exploration Phase Priority",
      "recommendation": "Immediately investigate Vast.ai deployment mechanism and existing codebase",
      "rationale": "HIGH RISK: Cannot design template system without understanding how templates are deployed, stored, and integrated with dashboard."
    },
    {
      "category": "Performance Testing",
      "recommendation": "Test boot times on actual Vast.ai hardware before finalizing",
      "rationale": "Network speeds, GPU initialization, and platform overhead may affect boot time estimates."
    }
  ],
  "critical_exploration_phase_tasks": [
    {
      "task": "Investigate existing Vast.ai integration",
      "questions": [
        "How are instances currently provisioned?",
        "Is there existing template/image management?",
        "What APIs are available for deployment?",
        "How does the dashboard communicate with Vast.ai?"
      ]
    },
    {
      "task": "Analyze current codebase architecture",
      "questions": [
        "Where would template selection UI live?",
        "How is GPU metadata currently accessed?",
        "What is current instance boot flow?",
        "Are there existing Docker integrations?"
      ]
    },
    {
      "task": "Validate CUDA/Docker setup on Vast.ai",
      "questions": [
        "What CUDA versions are pre-installed?",
        "Is nvidia-docker available?",
        "What is baseline boot time for simple container?",
        "Any platform-specific constraints?"
      ]
    },
    {
      "task": "Determine template storage strategy",
      "questions": [
        "Where will Docker images be stored (DockerHub, private registry)?",
        "How will templates be versioned?",
        "What is acceptable image size limit?",
        "How are templates distributed to instances?"
      ]
    }
  ],
  "research_limitations": {
    "web_tools_unavailable": "Context7 MCP, WebSearch, and WebFetch were not accessible",
    "documentation_not_accessed": "Could not verify against live official documentation",
    "vast_ai_unknown": "No information available about target platform integration",
    "confidence_level": "medium - based on industry standard knowledge but not verified"
  },
  "created_at": "2025-12-31T00:00:00Z"
}
